#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ submission.csv –Ω–∞ –æ—Å–Ω–æ–≤–µ test.csv

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ
–≤ HTTP –∑–∞–ø—Ä–æ—Å—ã –∫ Finam TradeAPI.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/generate_submission.py [OPTIONS]

–û–ø—Ü–∏–∏:
    --test-file PATH      –ü—É—Ç—å –∫ test.csv (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: data/processed/test.csv)
    --train-file PATH     –ü—É—Ç—å –∫ train.csv (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: data/processed/train.csv)
    --output-file PATH    –ü—É—Ç—å –∫ submission.csv (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: data/processed/submission.csv)
    --num-examples INT    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è few-shot (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)
    --batch-size INT      –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5)
"""

import csv
import os
import sys
import random
from pathlib import Path

import click
try:
    from tqdm import tqdm  # type: ignore[import-untyped]
except Exception:  # pragma: no cover
    def tqdm(iterable, desc: str = ""):
        return iterable

# Ensure project root is importable for `src` package
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from src.app.core.llm import call_llm
from src.app.core.prompting import system_prompt_for_mapping, endpoints_spec, symbols_spec
from src.app.core.normalize import parse_date_range
from src.app.core.symbols import SymbolResolver
from src.app.registry.endpoints import EndpointRegistry
from src.app.orchestration.extractor import extract_structured
from src.app.orchestration.endpoints import build_from_schema


def calculate_cost(usage: dict, model: str) -> float:
    """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ usage –∏ –º–æ–¥–µ–ª–∏"""
    # –¶–µ–Ω—ã OpenRouter (–ø—Ä–∏–º–µ—Ä–Ω—ã–µ, –≤ $ –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤)
    # –ò—Å—Ç–æ—á–Ω–∏–∫: https://openrouter.ai/models
    pricing = {
        "openai/gpt-4o-mini": {"prompt": 0.15, "completion": 0.60},
        "openai/gpt-4o": {"prompt": 2.50, "completion": 10.00},
        "openai/gpt-3.5-turbo": {"prompt": 0.50, "completion": 1.50},
        "anthropic/claude-3-sonnet": {"prompt": 3.00, "completion": 15.00},
        "anthropic/claude-3-haiku": {"prompt": 0.25, "completion": 1.25},
    }

    # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—ã –¥–ª—è –º–æ–¥–µ–ª–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∫–∞–∫ –¥–ª—è gpt-4o-mini)
    prices = pricing.get(model, {"prompt": 0.15, "completion": 0.60})

    prompt_tokens = usage.get("prompt_tokens", 0)
    completion_tokens = usage.get("completion_tokens", 0)

    # –°—á–∏—Ç–∞–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å (—Ü–µ–Ω–∞ –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤)
    prompt_cost = (prompt_tokens / 1_000_000) * prices["prompt"]
    completion_cost = (completion_tokens / 1_000_000) * prices["completion"]

    return prompt_cost + completion_cost


def _classify_request(path: str) -> str:
    if path.startswith("/v1/instruments/") and "/quotes/latest" in path:
        return "quotes"
    if path.startswith("/v1/instruments/") and "/orderbook" in path:
        return "orderbook"
    if path.startswith("/v1/instruments/") and "/bars" in path:
        return "bars"
    if path.startswith("/v1/instruments/") and "/trades/latest" in path:
        return "trades_latest"
    if path.startswith("/v1/accounts/") and "/orders" in path and path.count("/") == 4:
        return "orders_list"
    if path.startswith("/v1/accounts/") and "/orders/" in path and path.count("/") == 5:
        return "order_get"
    if path.startswith("/v1/accounts/") and "/trades" in path:
        return "account_trades"
    if path.startswith("/v1/accounts/") and "/transactions" in path:
        return "transactions"
    if path.startswith("/v1/assets/") and "/params" in path:
        return "asset_params"
    if path.startswith("/v1/assets/") and "/options" in path:
        return "asset_options"
    if path == "/v1/assets":
        return "assets"
    if path == "/v1/exchanges":
        return "exchanges"
    if path == "/v1/sessions" or path == "/v1/sessions/details":
        return "sessions"
    return "other"


def load_train_examples(train_file: Path, num_examples: int = 10) -> list[dict[str, str]]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∏–∑ train.csv —Å —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–π endpoint'–æ–≤."""
    rows: list[dict[str, str]] = []
    with open(train_file, encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter=";")
        for row in reader:
            rows.append({"question": row["question"], "type": row["type"], "request": row["request"]})

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å—Ö–µ–º —Å –ø–æ–º–æ—â—å—é —Ä–µ–µ—Å—Ç—Ä–∞ (–ø–æ —à–∞–±–ª–æ–Ω–∞–º –ø—É—Ç–∏)
    reg = EndpointRegistry()
    by_cat: dict[str, list[dict[str, str]]] = {}
    for r in rows:
        schema = reg.classify_path(r["request"]) or _classify_request(r["request"])  # fallback
        by_cat.setdefault(schema, []).append(r)

    # –ñ–µ–ª–∞–µ–º–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Äî —Å—Ö–µ–º—ã –∏–∑ —Ä–µ–µ—Å—Ç—Ä–∞, –∑–∞—Ç–µ–º –ø—Ä–æ—á–µ–µ
    priority: list[str] = []
    import yaml  # type: ignore
    with open(reg.config_path, encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    for it in cfg.get("endpoints", []):
        priority.append(it.get("schema"))
    priority.append("other")

    selected: list[dict[str, str]] = []
    per_cat = max(1, num_examples // max(1, len(priority)))
    for cat in priority:
        pool = by_cat.get(cat, [])
        if not pool:
            continue
        k = min(per_cat, len(pool))
        selected.extend(random.sample(pool, k))

    # –ï—Å–ª–∏ –Ω–µ –¥–æ–±—Ä–∞–ª–∏, –¥–æ–∑–∞–ø–æ–ª–Ω–∏–º –∏–∑ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è
    if len(selected) < num_examples:
        rest = [r for r in rows if r not in selected]
        k = min(num_examples - len(selected), len(rest))
        if k > 0:
            selected.extend(random.sample(rest, k))

    return selected[:num_examples]


def _collect_symbols_from_train(train_file: Path) -> list[str]:
    syms: list[str] = []
    import re as _re
    with open(train_file, encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter=";")
        for row in reader:
            q = (row.get("question") or "")
            for tok in set(_re.findall(r"\b[A-Z0-9]{2,12}(?:@[A-Z]{2,8})?\b", q.upper())):
                if _re.fullmatch(r"ORD\d+", tok):
                    continue
                if tok.isdigit():
                    continue
                syms.append(tok)
    return syms


def create_prompt(question: str, examples: list[dict[str, str]], train_file: Path) -> str:
    """–°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM —Å few-shot –ø—Ä–∏–º–µ—Ä–∞–º–∏"""
    prompt = (
        system_prompt_for_mapping()
        + "\n\n"
        + endpoints_spec()
        + "\n\n"
        + symbols_spec(_collect_symbols_from_train(train_file))
        + "\n\n–ü—Ä–∏–º–µ—Ä—ã:\n\n"
    )

    for ex in examples:
        prompt += f'–í–æ–ø—Ä–æ—Å: "{ex["question"]}"\n'
        prompt += f"–û—Ç–≤–µ—Ç: {ex['type']} {ex['request']}\n\n"

    prompt += f'–í–æ–ø—Ä–æ—Å: "{question}"\n'
    prompt += "–û—Ç–≤–µ—Ç (—Ç–æ–ª—å–∫–æ HTTP –º–µ—Ç–æ–¥ –∏ –ø—É—Ç—å, –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π):"

    return prompt


def parse_llm_response(response: str) -> tuple[str, str]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ LLM –≤ (type, request)"""
    response = response.strip()

    # –ò—â–µ–º HTTP –º–µ—Ç–æ–¥ –≤ –Ω–∞—á–∞–ª–µ
    methods = ["GET", "POST", "DELETE", "PUT", "PATCH"]
    method = "GET"  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    request = response

    for m in methods:
        if response.upper().startswith(m):
            method = m
            request = response[len(m) :].strip()
            break

    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    request = request.strip()
    if not request.startswith("/"):
        # –ï—Å–ª–∏ LLM –≤–µ—Ä–Ω—É–ª —á—Ç–æ-—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ–µ, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø—É—Ç—å
        parts = request.split()
        for part in parts:
            if part.startswith("/"):
                request = part
                break

    # Fallback –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
    if not request.startswith("/"):
        request = "/v1/assets"

    return method, request


def _extract_account_id(text: str) -> str | None:
    import re
    U = (text or "").upper()
    # FIN-203-B, ACC-001-A, USR-305-C
    m = re.search(r"\b(?:ACC|USR|FIN)-\d{3}-[A-Z]\b", U)
    if m:
        return m.group(0)
    # A12345, B98765, C54321, etc.
    m = re.search(r"\b[A-Z]\d{5,}\b", U)
    if m:
        return m.group(0)
    # plain number (last resort)
    m = re.search(r"\b\d{3,}\b", U)
    if m:
        return m.group(0)
    return None


def _extract_order_id(text: str) -> str | None:
    import re
    U = (text or "").upper()
    m = re.search(r"\bORD[A-Z0-9-]*\b", U)
    if m:
        return m.group(0)
    return None


def _extract_symbol(text: str) -> str | None:
    import re
    U = (text or "").upper()
    # Prefer tokens with '@' first
    tokens = re.findall(r"\b[A-Z0-9]{2,12}(?:@[A-Z]{2,8})?\b", U)
    for tok in tokens:
        if tok.startswith("ORD"):
            continue
        if "@" in tok:
            return tok
    for tok in tokens:
        if tok.startswith("ORD"):
            continue
        return tok
    return None


def _postprocess_request(question: str, method: str, path: str) -> str:
    """Fill placeholders in the generated path using cues from the question."""
    if not path:
        return path
    acc = _extract_account_id(question)
    if acc:
        path = path.replace("{account_id}", acc)
        # also replace query param placeholder if present
        path = path.replace("account_id={account_id}", f"account_id={acc}")
    order_id = _extract_order_id(question)
    if order_id:
        path = path.replace("{order_id}", order_id)
    sym = _extract_symbol(question)
    if sym:
        path = path.replace("/{symbol}", f"/{sym}")
    # Handle {slot} for intervals using parse_date_range
    if "{slot}" in path:
        rng = parse_date_range(question)
        if rng:
            start, end = rng
            # replace first then second occurrence
            if "interval.start_time={slot}" in path:
                path = path.replace("interval.start_time={slot}", f"interval.start_time={start}")
            else:
                path = path.replace("{slot}", start, 1)
            if "interval.end_time={slot}" in path:
                path = path.replace("interval.end_time={slot}", f"interval.end_time={end}")
            else:
                path = path.replace("{slot}", end, 1)
    return path


def generate_api_call(question: str, examples: list[dict[str, str]], model: str, train_file: Path, force_llm: bool = False) -> tuple[dict[str, str], float]:
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å API –∑–∞–ø—Ä–æ—Å –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞

    Returns:
        tuple: (result_dict, cost_in_dollars)
    """
    # First try: structured extractor (deterministic) unless LLM is forced
    if not force_llm:
        schema, missing = extract_structured(question)
        if schema and not missing:
            method, path, params = build_from_schema(schema)
            if params:
                # inline query if needed for leaderboard path
                if method == "GET" and path and "?" not in path:
                    query = "&".join(f"{k}={v}" for k, v in params.items())
                    path = f"{path}?{query}"
            # post-process placeholders
            path = _postprocess_request(question, method, path)
            return {"type": method, "request": path, "_source": "structured"}, 0.0

    # Fallback: LLM prompt mapping
    prompt = create_prompt(question, examples, train_file)
    messages = [{"role": "user", "content": prompt}]
    try:
        response = call_llm(messages, temperature=0.0, max_tokens=200)
        llm_answer = response["choices"][0]["message"]["content"].strip()
        method, request = parse_llm_response(llm_answer)
        # Validate via registry classification and fallback to offline mapping when invalid
        reg = EndpointRegistry()
        if not request.startswith("/") or reg.classify_path(request) is None:
            # Try deterministic offline mapping
            from src.app.leaderboard.offline_map import offline_map
            mapped = offline_map(question)
            if mapped:
                method, request = mapped
        usage = response.get("usage", {})
        cost = calculate_cost(usage, model)
        # post-process placeholders
        request = _postprocess_request(question, method, request)
        # mark source for debugging
        src = "llm" if reg.classify_path(request) is not None else "offline"
        return {"type": method, "request": request, "_llm": llm_answer, "_source": src}, cost

    except Exception as e:
        click.echo(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ '{question[:50]}...': {e}", err=True)
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback
        return {"type": "GET", "request": "/v1/assets", "_llm": "", "_source": "fallback"}, 0.0


@click.command()
@click.option(
    "--test-file",
    type=click.Path(exists=True, path_type=Path),
    default="data/processed/test.csv",
    help="–ü—É—Ç—å –∫ test.csv",
)
@click.option(
    "--train-file",
    type=click.Path(exists=True, path_type=Path),
    default="data/processed/train.csv",
    help="–ü—É—Ç—å –∫ train.csv",
)
@click.option(
    "--output-file",
    type=click.Path(path_type=Path),
    default="data/processed/submission.csv",
    help="–ü—É—Ç—å –∫ submission.csv",
)
@click.option("--num-examples", type=int, default=10, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è few-shot")
@click.option("--debug", is_flag=True, default=False, help="–ü–µ—á–∞—Ç—å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ train –∏ —Ç–æ–ø –æ—à–∏–±–æ–∫")
@click.option("--errors-out", type=click.Path(path_type=Path), default=None, help="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—à–∏–±–∫–∏ –≤ CSV")
@click.option("--force-llm", is_flag=True, default=False, help="–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ª–æ–π –∏ –≤—Å–µ–≥–¥–∞ –≤—ã–∑—ã–≤–∞—Ç—å LLM")
def main(test_file: Path, train_file: Path, output_file: Path, num_examples: int, debug: bool, errors_out: Path | None, force_llm: bool) -> None:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è submission.csv –¥–ª—è —Ö–∞–∫–∞—Ç–æ–Ω–∞"""
    from src.app.core.config import get_settings

    click.echo("üöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è submission —Ñ–∞–π–ª–∞...")
    click.echo(f"üìñ –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {train_file}...")

    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    settings = get_settings()
    model = settings.openrouter_model

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è few-shot
    examples = load_train_examples(train_file, num_examples)
    click.echo(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è few-shot learning")
    click.echo(f"ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å: {model}")

    # –ß–∏—Ç–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä
    click.echo(f"üìñ –ß—Ç–µ–Ω–∏–µ {test_file}...")
    test_questions = []
    with open(test_file, encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter=";")
        for row in reader:
            test_questions.append({"uid": row["uid"], "question": row["question"]})

    click.echo(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(test_questions)} –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã
    click.echo("\nü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è API –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é LLM...")
    results = []
    total_cost = 0.0

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm —Å postfix –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏
    progress_bar = tqdm(test_questions, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞")
    for item in progress_bar:
        api_call, cost = generate_api_call(item["question"], examples, model, train_file, force_llm=force_llm)
        total_cost += cost
        results.append({
            "uid": item["uid"],
            "type": api_call["type"],
            "request": api_call["request"],
            "_llm": api_call.get("_llm", ""),
            "_source": api_call.get("_source", "")
        })

        # –û–±–Ω–æ–≤–ª—è–µ–º postfix —Å —Ç–µ–∫—É—â–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é
        progress_bar.set_postfix({"cost": f"${total_cost:.4f}"})

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ submission.csv
    click.echo(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {output_file}...")
    output_file.parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=["uid", "type", "request"],
            delimiter=";",
            lineterminator="\n",
        )
        writer.writeheader()
        writer.writerows([{k: r[k] for k in ("uid","type","request")} for r in results])

    click.echo(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ–∑–¥–∞–Ω–æ {len(results)} –∑–∞–ø–∏—Å–µ–π –≤ {output_file}")
    click.echo(f"\nüí∞ –û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: ${total_cost:.4f}")
    click.echo(f"   –°—Ä–µ–¥–Ω—è—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –Ω–∞ –∑–∞–ø—Ä–æ—Å: ${total_cost / len(results):.6f}")
    click.echo("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤:")
    type_counts: dict[str, int] = {}
    for r in results:
        type_counts[r["type"]] = type_counts.get(r["type"], 0) + 1
    for method, count in sorted(type_counts.items()):
        click.echo(f"  {method}: {count}")
    # source stats
    src_counts: dict[str, int] = {}
    for r in results:
        src = r.get("_source", "")
        if src:
            src_counts[src] = src_counts.get(src, 0) + 1
    if src_counts:
        click.echo("\nüîé –ò—Å—Ç–æ—á–Ω–∏–∫ –ø—É—Ç–µ–π:")
        for s,c in sorted(src_counts.items()):
            click.echo(f"  {s}: {c}")

    # Optional debug evaluation when test_file is train_file
    if debug:
        click.echo("\nüîé Debug: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å train –∏ —Ä–∞–∑–±–æ—Ä –æ—à–∏–±–æ–∫")
        # Load truth
        truth: dict[str, dict[str, str]] = {}
        def _normalize_path(s: str) -> str:
            s = (s or "").strip()
            U = s.upper()
            for m in ("GET ", "POST ", "DELETE ", "PUT ", "PATCH "):
                if U.startswith(m):
                    return s[len(m):].strip()
            return s
        with open(train_file, encoding="utf-8") as f:
            rdr = csv.DictReader(f, delimiter=';')
            for row in rdr:
                # normalize request in truth: some rows have METHOD duplicated in request
                row_norm = dict(row)
                row_norm['request'] = _normalize_path(row.get('request',''))
                truth[row['uid']] = row_norm
        reg = EndpointRegistry()
        total = len(results)
        exact = 0
        type_ok = 0
        by_schema: dict[str, dict[str,int]] = {}
        errors: list[dict[str,str]] = []
        for r in results:
            uid = r['uid']
            pred_t, pred_p = r['type'], r['request']
            t = truth.get(uid, {})
            true_t, true_p = t.get('type',''), t.get('request','')
            schema = reg.classify_path(true_p) or 'other'
            rec = by_schema.setdefault(schema, {"total":0,"exact":0})
            rec["total"] += 1
            if pred_t == true_t:
                type_ok += 1
            if pred_t == true_t and pred_p == true_p:
                exact += 1
                rec["exact"] += 1
            else:
                errors.append({
                    "uid": uid,
                    "question": truth.get(uid, {}).get('question',''),
                    "true_type": true_t,
                    "true_request": true_p,
                    "pred_type": pred_t,
                    "pred_request": pred_p,
                    "schema": schema,
                    "llm": r.get('_llm',''),
                    "source": r.get('_source','')
                })
        click.echo(f"Total: {total}")
        click.echo(f"Exact matches: {exact}")
        click.echo(f"Type matches: {type_ok}")
        click.echo(f"Mismatches: {len(errors)}")
        click.echo("Per‚Äëschema accuracy:")
        for sc,m in by_schema.items():
            acc = (m.get('exact',0)/max(1,m.get('total',0)))*100
            click.echo(f"  {sc}: {m.get('exact',0)}/{m.get('total',0)} = {acc:.1f}%")
        # Show top 20 errors
        for e in errors[:20]:
            click.echo(f"- [{e['schema']}] Q: {e['question']}\n  true: {e['true_type']} {e['true_request']}\n  pred: {e['pred_type']} {e['pred_request']}\n  llm: {e['llm'][:200]}")
        if errors_out:
            with open(errors_out, 'w', encoding='utf-8', newline='') as f:
                w = csv.DictWriter(f, fieldnames=list(errors[0].keys()) if errors else ['uid','question','true_type','true_request','pred_type','pred_request','schema','llm'])
                w.writeheader()
                w.writerows(errors)
            click.echo(f"üìù –û—à–∏–±–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {errors_out}")


if __name__ == "__main__":
    main()
